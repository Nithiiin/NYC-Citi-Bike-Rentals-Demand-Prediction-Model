{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lpx0Fbq7H6bj","executionInfo":{"status":"ok","timestamp":1702378114730,"user_tz":360,"elapsed":26234,"user":{"displayName":"Ayse Dogan","userId":"17299110129555462884"}},"outputId":"c5a0bbfa-2b51-43a7-bd2b-e9900c57377c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVKfEtr4Ho7h"},"outputs":[],"source":["import pickle\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","source":["\n","X_test_filename = '/content/drive/MyDrive/IE434 Deep Dive 11 project/Deep Dive 4 (Milestone 3)/X_test.pkl'\n","\n","with open(X_test_filename, 'rb') as file:\n","    X_test = pickle.load(file)\n","\n","y_test_filename = '/content/drive/MyDrive/IE434 Deep Dive 11 project/Deep Dive 4 (Milestone 3)/y_test.pkl'\n","\n","with open(y_test_filename, 'rb') as file:\n","    y_test = pickle.load(file)\n","\n","y_test_df = '/content/drive/MyDrive/IE434 Deep Dive 11 project/Deep Dive 4 (Milestone 3)/y_test_numerical.pkl'\n","\n","with open(y_test_df, 'rb') as file:\n","    y_test_df = pickle.load(file)"],"metadata":{"id":"bbc2e5REHtyE","colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"status":"error","timestamp":1702378121290,"user_tz":360,"elapsed":19,"user":{"displayName":"Ayse Dogan","userId":"17299110129555462884"}},"outputId":"24e77b2d-d6e7-4182-fc08-9cbcb4fee6d8"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-16d0134e8d8a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_test_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/IE434 Deep Dive 11 project/Deep Dive 4 (Milestone 3)/X_test.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/IE434 Deep Dive 11 project/Deep Dive 4 (Milestone 3)/X_test.pkl'"]}]},{"cell_type":"markdown","source":["\n","\n","# ImprovedGRUModel Class\n","\n","The `ImprovedGRUModel` class is a PyTorch neural network module, inheriting from `nn.Module`. It is designed to implement an improved version of the GRU (Gated Recurrent Unit) model, suitable for sequence modeling tasks. The class is structured as follows:\n","\n","- Initializes the model with the following parameters:\n","  - `input_size`: The number of expected features in the input `x`.\n","  - `hidden_size`: The number of features in the hidden state `h`.\n","  - `output_size`: The number of features in the output.\n","  - `num_layers`: Number of recurrent layers (default is 1).\n","  - `dropout_rate`: If non-zero, introduces a dropout layer on the outputs of each GRU layer except the last layer (default is 0.2).\n","- The constructor sets up the following layers:\n","  - A GRU layer.\n","  - A dropout layer.\n","  - A fully connected (linear) layer.\n","\n","## GRU Layer\n","- Defined using `nn.GRU`, it processes the input sequence.\n","- Applies dropout between layers if `num_layers` is more than 1.\n","\n","## Dropout Layer\n","- Implemented using `nn.Dropout`, it randomly zeros some of the elements of the input tensor with probability `dropout_rate`.\n","\n","## Fully Connected Layer\n","- A linear layer (`nn.Linear`) that maps the hidden state output of the GRU to the output size.\n","\n","## Method: `forward(self, x, hidden)`\n","- Defines the forward pass of the model.\n","- Takes `x` (input) and `hidden` (initial hidden state) as inputs.\n","- Applies the GRU layer, followed by dropout, and then passes the output through the fully connected layer.\n","- Returns the output and the hidden state.\n","\n","## Method: `init_hidden(self, batch_size)`\n","- Initializes the hidden state of the GRU.\n","- Returns a tensor of zeros with shape `(num_layers, batch_size, hidden_size)`.\n"],"metadata":{"id":"dqQiG1eTwdoU"}},{"cell_type":"code","source":["class ImprovedGRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout_rate=0.2):\n","        super(ImprovedGRUModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        # GRU Layer\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=(dropout_rate if num_layers > 1 else 0))\n","\n","        # Dropout layer\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","        # Fully connected Layer\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, hidden):\n","        out, hidden = self.gru(x, hidden)\n","\n","        # Applying dropout to the output of the last GRU layer\n","        out = self.dropout(out)\n","\n","        # Reshape output for the fully connected layer\n","        out = out.reshape(-1, self.hidden_size)\n","\n","        # Get the final output\n","        out = self.fc(out)\n","\n","        return out, hidden\n","\n","    def init_hidden(self, batch_size):\n","        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n","        return hidden\n"],"metadata":{"id":"K9X72dSBJHDJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","## Function: `calculate_accuracy(conf_matrix)`\n","This function calculates the accuracy of a model based on a confusion matrix.\n","- **Parameter:** `conf_matrix` - a confusion matrix of the model predictions.\n","- **Returns:** The accuracy, calculated as the ratio of correctly predicted instances (the trace of the confusion matrix) to the total number of instances (the sum of all elements in the confusion matrix).\n","\n","## Function: `evaluate_feature_importance(model, X_test_tensor, y_test_tensor, batch_size=64, call_time=1)`\n","This function evaluates the importance of each feature in a dataset for a given model's predictions.\n","- **Parameters:**\n","  - `model`: The trained model to evaluate.\n","  - `X_test_tensor`: Test data features as a tensor.\n","  - `y_test_tensor`: Test data labels as a tensor.\n","  - `batch_size`: Batch size for the DataLoader (default is 64).\n","  - `call_time`: A seed for reproducibility of results (default is 1).\n","- **Process:**\n","  - First, the function calculates the model's accuracy on the original test data without any alteration.\n","  - It then iteratively shuffles each feature (column) of the test data, evaluates the model's accuracy with this shuffled data, and records the accuracy.\n","  - A decrease in accuracy after shuffling a particular feature implies the importance of that feature for the model's predictions.\n","- **Visualizations:**\n","  - It plots two graphs:\n","    - The first graph shows the accuracy for each shuffled column compared to the original accuracy.\n","    - The second graph displays the decrease in accuracy for each shuffled column.\n","- **Returns:** A tuple containing:\n","  - `column_accuracies`: A list of accuracies for each shuffled column.\n","  - `accuracy_decrease`: A list representing the decrease in accuracy for each shuffled column compared to the original accuracy.\n","\n","### Additional Notes:\n","- The function uses PyTorch's `DataLoader` and `TensorDataset` for batch processing of the test data.\n","- `torch.no_grad()` is used to disable gradient calculations, saving memory and computations during the evaluation phase.\n","- The function uses `confusion_matrix` (likely from a library like scikit-learn) to calculate the confusion matrix for each test set variant.\n","- The visualization is performed using matplotlib, a popular plotting library in Python.\n","\n","This function is useful for understanding which features are most influential for the model's predictions, which can be crucial for feature selection and model interpretation."],"metadata":{"id":"GGEkFDwq7_xT"}},{"cell_type":"code","source":["def calculate_accuracy(conf_matrix):\n","    return np.trace(conf_matrix) / np.sum(conf_matrix)\n","\n","def evaluate_feature_importance(model, X_test_tensor, y_test_tensor, batch_size=64, call_time=1):\n","    # DataLoader for test set\n","    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, shuffle=False)\n","\n","    # confusion matrix and accuracy on ORIGINAL test data\n","    model.eval()\n","    all_predictions = []\n","    all_true_labels = []\n","    with torch.no_grad():\n","        for X_batch, y_batch in test_loader:\n","            # launch model and prediction\n","            batch_size = X_batch.size(0)\n","            hidden = model.init_hidden(batch_size)\n","            output, hidden = model(X_batch.unsqueeze(1), hidden)\n","            predictions = output.argmax(dim=1)\n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_true_labels.extend(y_batch.cpu().numpy())\n","\n","    original_conf_matrix = confusion_matrix(all_true_labels, all_predictions)\n","    original_accuracy = calculate_accuracy(original_conf_matrix)\n","\n","    # python list to store accuracy based on each row's permutation\n","    column_accuracies = []\n","\n","    # Permutation of each columns of X_test_tensor\n","    torch.manual_seed(call_time)\n","    for column in range(X_test_tensor.size(1)):\n","        shuffled_X_test = X_test_tensor.clone()\n","        shuffled_indices = torch.randperm(shuffled_X_test.size(0))\n","        shuffled_X_test[:, column] = shuffled_X_test[shuffled_indices, column]\n","\n","        # DataLoader for shuffled test set\n","        shuffled_test_loader = DataLoader(TensorDataset(shuffled_X_test, y_test_tensor), batch_size=batch_size, shuffle=False)\n","\n","        # launch model and confusion matrix based on newly shuffled dataset\n","        all_predictions = []\n","        all_true_labels = []\n","        with torch.no_grad():\n","            for X_batch, y_batch in shuffled_test_loader:\n","                batch_size = X_batch.size(0)\n","                hidden = model.init_hidden(batch_size)\n","                output, hidden = model(X_batch.unsqueeze(1), hidden)\n","                predictions = output.argmax(dim=1)\n","                all_predictions.extend(predictions.cpu().numpy())\n","                all_true_labels.extend(y_batch.cpu().numpy())\n","\n","        conf_matrix = confusion_matrix(all_true_labels, all_predictions)\n","        accuracy = calculate_accuracy(conf_matrix)\n","        column_accuracies.append(accuracy)\n","\n","    # Visualize Accuracy\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(column_accuracies, label='Shuffled Column Accuracy')\n","    plt.axhline(y=original_accuracy, color='r', linestyle='--', label='Original Accuracy')\n","    plt.xlabel('Column Index')\n","    plt.ylabel('Accuracy')\n","    plt.title('Accuracy per Shuffled Column')\n","    plt.legend()\n","    plt.show()\n","\n","    # Visualize Accuracy Descent by permutation\n","    accuracy_decrease = [original_accuracy - acc for acc in column_accuracies]\n","    decreased_order_accuracy_decrease = sorted(accuracy_decrease)\n","    plt.figure(figsize=(12, 6))\n","    plt.bar(range(len(accuracy_decrease)), accuracy_decrease)\n","    plt.xlabel('Column Index')\n","    plt.ylabel('Decrease in Accuracy')\n","    plt.title('Decrease in Accuracy per Shuffled Column')\n","    plt.show()\n","\n","    return column_accuracies, accuracy_decrease"],"metadata":{"id":"-jU6LZk3iDuY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Convert X_test DataFrame to PyTorch tensor\n","X_test_tensor = torch.tensor(X_test.values).float()\n","y_test_tensor = torch.tensor(y_test_df.values).long()  # Directly use y_test if it's already encoded\n","\n","# Load the model\n","model_GRU = torch.load('/content/drive/MyDrive/IE434 Deep Dive 11 project/Deep Dive 4 (Milestone 3)/best_model_GRU.pth')\n","model_GRU.eval()\n","\n","# call function to evaluate feature importance\n","column_accuracies, accuracy_decrease = evaluate_feature_importance(model_GRU, X_test_tensor, y_test_tensor, batch_size=64)\n"],"metadata":{"id":"bAyDdKiHpULQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def average_lists(lists):\n","    \"\"\" Function to calculate the average of given lists \"\"\"\n","    # Sum the corresponding elements of the lists\n","    summed = [sum(values) for values in zip(*lists)]\n","    # Calculate the average\n","    return [s / len(lists) for s in summed]\n","\n","# Number of calls\n","num_calls = 1\n","\n","# Lists to store list of accuracies, accuracy_decrease\n","all_column_accuracies = []\n","all_accuracy_decreases = []\n","\n","# Call the function multiple times and store the results\n","for _ in range(num_calls):\n","    column_accuracies, accuracy_decrease = evaluate_feature_importance(model_GRU, X_test_tensor, y_test_tensor, batch_size=64,call_time=num_calls)\n","    all_column_accuracies.append(column_accuracies)\n","    all_accuracy_decreases.append(accuracy_decrease)\n","\n","# Calculate the averages\n","avg_column_accuracies = average_lists(all_column_accuracies)\n","avg_accuracy_decrease = average_lists(all_accuracy_decreases)\n","\n","# Print the results\n","print(\"Average Column Accuracies:\", avg_column_accuracies)\n","print(\"Average Accuracy Decrease:\", avg_accuracy_decrease)\n","\n"],"metadata":{"id":"FoXnHv6T59G9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Average Column Accuracy Decrease\n","avg_accuracy_decrease = pd.DataFrame(avg_accuracy_decrease).T\n","avg_accuracy_decrease.columns = X_test.columns\n","avg_accuracy_decrease = avg_accuracy_decrease.T\n","avg_accuracy_decrease = avg_accuracy_decrease.sort_values(by=0, ascending=False)\n","avg_accuracy_decrease = avg_accuracy_decrease.T\n","plt.figure(figsize=(10, 6))\n","plt.bar(avg_accuracy_decrease.columns, avg_accuracy_decrease.iloc[0])\n","plt.xlabel('Permutated Columns')\n","plt.ylabel('Accuracy Decrease')\n","plt.title('Feature Importance by permutating each feature')\n","plt.xticks(rotation=90)\n","plt.show()"],"metadata":{"id":"0IIPz170EM3k"},"execution_count":null,"outputs":[]}]}