{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup:"
      ],
      "metadata": {
        "id": "ZfGUIG_WNAoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oBVpzepR7G04",
        "outputId": "b1403943-edac-407e-d3cd-55b1ce32c86e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data File Location\n",
        "\n",
        "df = \"/content/drive/My Drive/IE434 Deep Dive 11 project/Deep Dive 3 (Milestone 2)\"\n",
        "filename = '/content/drive/My Drive/IE434 Deep Dive 11 project/Deep Dive 3 (Milestone 2)/working_df.pkl'\n"
      ],
      "metadata": {
        "id": "8BND_erC71PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOSC9Bs45GSF",
        "outputId": "d8817e99-037f-4fe9-a1bf-92a1083e1715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5e6230def030>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mworking_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/IE434 Deep Dive 11 project/Deep Dive 3 (Milestone 2)/working_df.pkl'"
          ]
        }
      ],
      "source": [
        "# Loading the Dataset\n",
        "\n",
        "with open(filename, 'rb') as file:\n",
        "    working_df = pickle.load(file)\n",
        "\n",
        "# Print the first few entries of the DataFrame\n",
        "\n",
        "print(working_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Missing Data:"
      ],
      "metadata": {
        "id": "2CQRmY-EM5yM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output below presents the initial number of rows in the dataset, followed by a detailed account of null values in each column.\n",
        "\n",
        "Initially, we had a total of 1,446,166 rows before any data cleansing. Upon inspection of individual columns, we observed that most columns have no missing data. Notable exceptions include `start_station_name`, `start_station_id`, `end_station_name`, `end_station_id`, `end_lat`, and `end_lng`, with a small percentage of missing values ranging from 0.01% to 0.34%.\n",
        "\n",
        "Given the low percentage of missing values relative to the size of the dataset, we decided to proceed with dropping rows with any null values. This decision is under the assumption that the loss of data is minimal and that maintaining a complete case analysis is critical for our subsequent analysis.\n",
        "\n",
        "After dropping these rows, we are left with a total of 1,441,178 rows. The impact of this operation was the removal of approximately 0.03% of the data, indicating a negligible reduction in dataset size while potentially improving data quality.\n",
        "\n",
        "Here is the summarized information:\n",
        "- Total rows before dropping null values: 1,446,166\n",
        "- Total rows after dropping null values: 1,441,178\n",
        "\n",
        "Below is the detailed breakdown of missing values before the operation:"
      ],
      "metadata": {
        "id": "OPAgI6fHN_Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total rows\n",
        "total_rows = len(working_df)\n",
        "print(f\"--- Total rows before dropping null values: {total_rows}\\n\")\n",
        "\n",
        "# Calculate number and percentage of null values for each column\n",
        "print(\"Percentage and number of null values in each column: \\n\")\n",
        "null_counts = working_df.isnull().sum()\n",
        "for column, count in null_counts.items():\n",
        "    percentage = (count / total_rows) * 100\n",
        "    print(f\"({percentage:.2f}%) : {count} null values in '{column}' \")\n",
        "print(\" \")\n",
        "\n",
        "# Drop rows containing any null value\n",
        "working_df.dropna(inplace=True)\n",
        "\n",
        "# Print total number of rows after dropping\n",
        "print(f\"--- Total rows after dropping null values: {len(working_df)}\")\n"
      ],
      "metadata": {
        "id": "8EMuLIf1JER-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping rows with missing values can sometimes lead to biases if the missing data is not randomly distributed. In this case, the missing data constituted a very small proportion of the total, suggesting that the impact on the overall dataset might be limited.\n"
      ],
      "metadata": {
        "id": "Yat_KHsmN3gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "working_df.dropna()\n",
        "\n",
        "# 1441178 rows Ã— 13 columns"
      ],
      "metadata": {
        "id": "9-8CVs1izfoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Average Coordinates for Stations\n",
        "\n",
        "To facilitate any geospatial analysis that might be required later on, we calculate the average latitude and longitude for each unique station in our dataset. This involves processing both the starting points and the destinations of the rides.\n",
        "\n",
        "## Start Stations\n",
        "First, we group the data by the `start_station_name` and calculate the mean latitude and longitude, creating a new DataFrame with these average coordinates.\n",
        "\n",
        "## End Stations\n",
        "Similarly, we repeat the process for `end_station_name` to get the average coordinates for all ending stations.\n",
        "\n",
        "## Combining Start and End Stations\n",
        "Since some stations serve as both start and end points, we combine the two sets of average coordinates into a single DataFrame. This helps us in deduplicating the stations and ensures that each unique station is represented only once.\n",
        "\n",
        "## Final Station Coordinates\n",
        "Lastly, we group by the station names again to ensure that if a station appears in both the start and end lists, we calculate the overall mean latitude and longitude for that station.\n"
      ],
      "metadata": {
        "id": "WpA-D6OQOsid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating avg of latitude and longitude for each of the unique start stations\n",
        "start_station_avg_coords = working_df.groupby('start_station_name').agg({'start_lat': 'mean', 'start_lng': 'mean'}).reset_index()\n",
        "\n",
        "start_station_avg_coords.rename(columns={'start_station_name':'station_name','start_lat': 'avg_latitude', 'start_lng': 'avg_longitude'}, inplace=True)\n",
        "\n",
        "#calculating avg of latitude and longitude for each of the unique end stations\n",
        "end_station_avg_coords = working_df.groupby('end_station_name').agg({'end_lat': 'mean', 'end_lng': 'mean'}).reset_index()\n",
        "\n",
        "end_station_avg_coords.rename(columns={'end_station_name':'station_name','end_lat': 'avg_latitude', 'end_lng': 'avg_longitude'}, inplace=True)\n",
        "\n",
        "#combining the start and end stations to find the unique number of stations with its avg lat and long.\n",
        "combined_stations = pd.concat([start_station_avg_coords, end_station_avg_coords])\n",
        "\n",
        "# Group by station name and calculate the mean for latitude and longitude\n",
        "union_station_coords = combined_stations.groupby('station_name', as_index=False).mean()\n",
        "\n",
        "print(union_station_coords)"
      ],
      "metadata": {
        "id": "_0iFAYURYX0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Updating Station Coordinates with Averaged Values\n",
        "\n",
        "To ensure consistency and accuracy in our geospatial data, we will update the original latitude and longitude of the start and end stations in our dataset using the averaged coordinates previously calculated.\n",
        "\n",
        "## Process for Start Stations\n",
        "We begin by merging the original DataFrame with the averaged coordinates DataFrame on the start station names. This adds the averaged latitude and longitude to each corresponding row.\n",
        "\n",
        "Next, we update the original start station coordinates with these averaged values. This is done using the `combine_first` method, which fills in the original values only if the averaged value is NaN.\n",
        "\n",
        "After updating the coordinates, we remove the intermediate columns that were added during the merge, as they are no longer necessary.\n",
        "\n",
        "## Process for End Stations\n",
        "We follow a similar process for the end stations, merging the intermediate DataFrame (with updated start station coordinates) with the averaged coordinates DataFrame on the end station names.\n",
        "\n",
        "Again, we update the original end station coordinates with the averaged values and drop the additional columns added during the merge.\n",
        "\n",
        "The final DataFrame, `final_df`, now contains the updated coordinates for both start and end stations.\n"
      ],
      "metadata": {
        "id": "A5q2_iJlPh8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge for start station latitude and longitude\n",
        "start_merged = working_df.merge(\n",
        "    union_station_coords,\n",
        "    how='left',\n",
        "    left_on='start_station_name',\n",
        "    right_on='station_name'\n",
        ").rename(columns={'avg_latitude': 'start_avg_lat', 'avg_longitude': 'start_avg_lng'})\n",
        "\n",
        "# Update the start station latitude and longitude\n",
        "start_merged['start_lat'] = start_merged['start_avg_lat'].combine_first(start_merged['start_lat'])\n",
        "start_merged['start_lng'] = start_merged['start_avg_lng'].combine_first(start_merged['start_lng'])\n",
        "\n",
        "# Dropping the additional columns from the first merge\n",
        "start_merged.drop(['station_name', 'start_avg_lat', 'start_avg_lng'], axis=1, inplace=True)\n",
        "\n",
        "# Merge for end station latitude and longitude\n",
        "end_merged = start_merged.merge(\n",
        "    union_station_coords,\n",
        "    how='left',\n",
        "    left_on='end_station_name',\n",
        "    right_on='station_name',\n",
        "    suffixes=('', '_end')\n",
        ").rename(columns={'avg_latitude': 'end_avg_lat', 'avg_longitude': 'end_avg_lng'})\n",
        "\n",
        "# Update the end station latitude and longitude\n",
        "end_merged['end_lat'] = end_merged['end_avg_lat'].combine_first(end_merged['end_lat'])\n",
        "end_merged['end_lng'] = end_merged['end_avg_lng'].combine_first(end_merged['end_lng'])\n",
        "\n",
        "# Dropping the additional columns from the second merge\n",
        "final_df = end_merged\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "ov2sayFVhee9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting Relevant Columns for Model Prediction\n",
        "\n",
        "To streamline the dataset for the upcoming modeling phase, we now reduce the number of features by selecting only those columns that are relevant for prediction. This step is critical to improve computational efficiency and to avoid having too many features that can actually degrade the performance of many machine learning models. Dropping unrelated features helps to focus the model on the most important information.\n"
      ],
      "metadata": {
        "id": "1RAnqBItWr8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping the unused columns for the model prediction\n",
        "columns_to_keep = ['rideable_type',\t'started_at',\t'start_station_name','end_station_name','ended_at','start_lat',\t'start_lng','end_lat','end_lng','member_casual']\n",
        "finalized_df = final_df[columns_to_keep]\n",
        "finalized_df.head()"
      ],
      "metadata": {
        "id": "Hn8tvZAwqriU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "working_df = finalized_df"
      ],
      "metadata": {
        "id": "NXAtqzht7RyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization:  Geospatial Data Distribution\n",
        "\n",
        "Understanding the distribution of geospatial data in our dataset is crucial for recognizing patterns and potential outliers.\n",
        "\n",
        "## Latitude Distribution\n",
        "We first examine the distribution of starting latitudes (`start_lat`). The histogram will show us how often certain latitude ranges occur, which can indicate the most common areas for the start of a ride.\n",
        "\n",
        "Next, we look at the distribution of ending latitudes (`end_lat`). Comparing this histogram with the starting latitude histogram can reveal whether certain areas are more commonly used as destinations.\n",
        "\n",
        "## Longitude Distribution\n",
        "Similarly, we analyze the distribution of starting longitudes (`start_lng`) and ending longitudes (`end_lng`). These histograms will help us to understand the spread of rides longitudinally, indicating the breadth of the area serviced by the rides.\n",
        "\n",
        "The histograms are plotted with a moderate number of bins (50) to provide a detailed yet comprehensible view of the data distribution. We also include grid lines for easier interpretation of the frequency values.\n"
      ],
      "metadata": {
        "id": "O8tScZMGI8jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "4\n",
        "# Histogram for start_lat\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(working_df['start_lat'], bins=50, color='blue', edgecolor='black')\n",
        "plt.title('Start Latitude Distribution')\n",
        "plt.xlabel('Start Latitude')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Histogram for start_lng\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(working_df['start_lng'], bins=50, color='green', edgecolor='black')\n",
        "plt.title('Start Longitude Distribution')\n",
        "plt.xlabel('Start Longitude')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Histogram for end_lat\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(working_df['end_lat'], bins=50, color='red', edgecolor='black')\n",
        "plt.title('End Latitude Distribution')\n",
        "plt.xlabel('End Latitude')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Histogram for end_lng\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(working_df['end_lng'], bins=50, color='purple', edgecolor='black')\n",
        "plt.title('End Longitude Distribution')\n",
        "plt.xlabel('End Longitude')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vu0inCkn6_vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histograms serve a crucial role in the initial exploratory data analysis, particularly in identifying outliers within the dataset. Outliers can be detected as data points that fall outside the overall pattern of distribution.\n",
        "\n",
        "To further refine the analysis, especially before deploying machine learning algorithms, standardization of these latitude and longitude values would be beneficial. Standardization (or Z-score normalization) transforms the data to have a mean of zero and a standard deviation of one. By standardizing the data, we improve the robustness and reliability of our models, allowing for better pattern recognition and outlier detection."
      ],
      "metadata": {
        "id": "QP9NytMiY_no"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DateTime Conversion and Duration Calculation"
      ],
      "metadata": {
        "id": "plNH5iNWaBgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'started_at' and 'ended_at' columns to datetime if they're not already\n",
        "working_df['started_at'] = pd.to_datetime(working_df['started_at'])\n",
        "working_df['ended_at'] = pd.to_datetime(working_df['ended_at'])\n",
        "\n",
        "# Calculate the duration of ride\n",
        "working_df['ride_duration'] = working_df['ended_at'] - working_df['started_at']\n",
        "\n",
        "# For duration in seconds\n",
        "working_df['duration_seconds'] = working_df['ride_duration'].dt.total_seconds()\n",
        "\n",
        "# Display the first few rows to confirm the new columns\n",
        "print(working_df[['started_at', 'ended_at', 'ride_duration', 'duration_seconds']].head())\n",
        "print(working_df['duration_seconds'].describe())"
      ],
      "metadata": {
        "id": "5ADp86qR7M_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization: Histogram of Ride Durations"
      ],
      "metadata": {
        "id": "jkGlhXhVaRDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a histogram of the 'duration_seconds' column\n",
        "plt.hist(working_df['duration_seconds'], bins=1000, range=[0, working_df['duration_seconds'].quantile(0.99)])\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Distribution of Ride Durations')\n",
        "plt.xlabel('Duration in Seconds')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HN9YipWt7ioj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The histogram illustrates the distribution of ride durations in seconds for a set of bike-sharing data. We can observe a right-skewed distribution, where the majority of rides are short in duration, clustering within the lower second range."
      ],
      "metadata": {
        "id": "t-gTa5diaxCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhancing Temporal Resolution in Data\n",
        "\n",
        "The code snippet focuses on refining the temporal aspects of the dataset. It extracts the precise date and constructs an hourly time interval to facilitate more granular analysis of usage patterns. Additionally, it introduces the 'day_of_week' and 'month' columns to enable trend analysis across different days and months, which could be pivotal for recognizing usage patterns and informing decisions based on temporal factors."
      ],
      "metadata": {
        "id": "U7DuWjUlbV3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the date as YYYY-MM-DD\n",
        "working_df['date'] = working_df['started_at'].dt.date\n",
        "\n",
        "# Create time interval by getting just the hour part of the 'started_at' and the next hour\n",
        "working_df['time_interval'] = working_df['started_at'].dt.strftime('%H') + \" - \" + \\\n",
        "                              (working_df['started_at'].dt.floor('H') + pd.Timedelta(hours=1)).dt.strftime('%H')\n",
        "\n",
        "# Ensure 'date' is in datetime format\n",
        "working_df['date'] = pd.to_datetime(working_df['date'])\n",
        "\n",
        "# Add a new column 'day_of_week' to get the day name\n",
        "working_df['day_of_week'] = working_df['date'].dt.day_name()\n",
        "working_df['month'] = working_df['date'].dt.month_name()\n",
        "\n",
        "# Now the 'day_of_week' column should contain the name of the day\n",
        "print(working_df[['date', 'time_interval', 'day_of_week', 'month']].head())\n"
      ],
      "metadata": {
        "id": "BB9bpL2Y7l5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization: Temporal Patterns in Ride Durations\n",
        "\n",
        "The provided code is a comprehensive approach to visualizing the temporal patterns in ride durations based on months and days of the week. It uses Seaborn and Matplotlib to create a series of bar plots, depicting both average and total ride durations. The data is first grouped by 'month' and 'day_of_week', with the mean and sum of 'duration_seconds' calculated for each group. The visualizations are designed to reveal trends and insights, such as which months or days are busiest or have the longest rides on average. Custom ordering of the x-axis ensures the data is presented in a logical sequence, from January to December and Monday to Sunday, respectively."
      ],
      "metadata": {
        "id": "7iPBk2JrbgfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import calendar\n",
        "\n",
        "# Set the overall aesthetic style of the plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Order the months from January to December\n",
        "month_order = [calendar.month_name[i] for i in range(1, 13)]\n",
        "\n",
        "# Order the weekdays starting from Monday\n",
        "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "\n",
        "# Assuming 'working_df' is your DataFrame with the 'month' and 'day_of_week' columns\n",
        "\n",
        "# Group and calculate mean and total durations for both month and day of week\n",
        "monthly_avg_durations = working_df.groupby('month')['duration_seconds'].mean().reindex(month_order)\n",
        "monthly_total_durations = working_df.groupby('month')['duration_seconds'].sum().reindex(month_order)\n",
        "daily_avg_durations = working_df.groupby('day_of_week')['duration_seconds'].mean().reindex(weekday_order)\n",
        "daily_total_durations = working_df.groupby('day_of_week')['duration_seconds'].sum().reindex(weekday_order)\n",
        "\n",
        "# Define a function for creating a formatted bar plot\n",
        "def create_bar_plot(x, y, title, xlabel, ylabel, x_rotation=45, figsize=(12, 6)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.barplot(x=x, y=y, order=x)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.xticks(rotation=x_rotation)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the average durations for each month\n",
        "create_bar_plot(monthly_avg_durations.index, monthly_avg_durations.values,\n",
        "                'Average Ride Duration per Month', 'Month', 'Average Duration in Seconds')\n",
        "\n",
        "# Plot the total durations for each month\n",
        "create_bar_plot(monthly_total_durations.index, monthly_total_durations.values,\n",
        "                'Total Ride Duration per Month', 'Month', 'Total Duration in Seconds')\n",
        "\n",
        "# Plot the average durations for each day of the week\n",
        "create_bar_plot(daily_avg_durations.index, daily_avg_durations.values,\n",
        "                'Average Ride Duration per Day of the Week', 'Day of the Week', 'Average Duration in Seconds')\n",
        "\n",
        "# Plot the total durations for each day of the week\n",
        "create_bar_plot(daily_total_durations.index, daily_total_durations.values,\n",
        "                'Total Ride Duration per Day of the Week', 'Day of the Week', 'Total Duration in Seconds')\n"
      ],
      "metadata": {
        "id": "7_wVgq067rjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume `columns_to_keep` is a list of column names that you want to keep\n",
        "columns_to_keep = ['rideable_type','member_casual',\t'start_lat','start_lng','start_station_name','end_station_name','date','day_of_week','month','time_interval',\t'end_lat','end_lng']  # replace with your actual column names\n",
        "\n",
        "# Now, select only these columns from `final_df`\n",
        "working_df = working_df[columns_to_keep]\n",
        "working_df"
      ],
      "metadata": {
        "id": "fmT_C9DM719c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming `df` is your DataFrame and 'date_column' is the name of your date column\n",
        "working_df['date'] = pd.to_datetime(working_df['date'])\n",
        "\n",
        "# Now format the dates in 'MM/DD/YYYY' format\n",
        "working_df['date'] = working_df['date'].dt.strftime('%m/%d/%Y')\n"
      ],
      "metadata": {
        "id": "u3xKx3Es8-vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "working_df.head()"
      ],
      "metadata": {
        "id": "jgeXiIY29OHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_filename = '/content/drive/My Drive/IE434 Deep Dive 11 project/Deep Dive 3 (Milestone 2)/weather.csv'"
      ],
      "metadata": {
        "id": "kUJ0Bwej-zqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df = pd.read_csv(weather_filename)\n",
        "weather_df.head()"
      ],
      "metadata": {
        "id": "zo_4dt8R-9xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dealt the missing values in the weather data by filling it with zeros and average values (average for temperature)"
      ],
      "metadata": {
        "id": "rMutPUXDZpB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename 'DATE' column to 'date' in weather_df\n",
        "weather_df.rename(columns={'DATE': 'date'}, inplace=True)\n",
        "\n",
        "# Convert 'DATE' column to datetime\n",
        "weather_df['date'] = pd.to_datetime(weather_df['date'], format='%m/%d/%Y')\n",
        "\n",
        "# Now, ensure that the 'date' column in working_df is also in datetime format\n",
        "working_df['date'] = pd.to_datetime(working_df['date'], format='%m/%d/%Y')\n",
        "\n",
        "\n",
        "final_df = working_df.merge(weather_df[['date', 'PRCP', 'SNOW', 'TMAX','TMIN']],\n",
        "                              on='date',\n",
        "                              how='left')\n",
        "\n",
        "# Assuming 'df' is your DataFrame and 'column_name' is the name of your column with missing values\n",
        "final_df['SNOW'] = final_df['SNOW'].fillna(0)\n",
        "\n",
        "TMAX_column_mean = final_df['TMAX'].mean()\n",
        "final_df['TMAX'] = final_df['TMAX'].fillna(TMAX_column_mean)\n",
        "\n",
        "TMIN_column_mean = final_df['TMIN'].mean()\n",
        "final_df['TMIN'] = final_df['TMIN'].fillna(TMIN_column_mean)\n",
        "\n",
        "final_df['PRCP'] = final_df['PRCP'].fillna(0)\n",
        "\n",
        "\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "PDO9qpL-_uQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.describe()"
      ],
      "metadata": {
        "id": "KWa9G2zrAW8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For all categorical columns\n",
        "categorical_columns = final_df.select_dtypes(include=['object', 'category']).columns\n",
        "final_df[categorical_columns].describe()\n"
      ],
      "metadata": {
        "id": "mOr02ZwSCVH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For each categorical column\n",
        "for col in categorical_columns:\n",
        "    if col == 'start_station_name' or col == 'end_station_name':\n",
        "      pass\n",
        "    else:\n",
        "      sns.countplot(y=col, data=final_df)\n",
        "      plt.show()\n"
      ],
      "metadata": {
        "id": "ZgnZ4v8nCr8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is your DataFrame\n",
        "final_df.to_pickle('/content/drive/My Drive/IE434 Deep Dive 11 project/Deep Dive 3 (Milestone 2)/final_df.pkl')\n",
        "union_station_coords.to_pickle('/content/drive/My Drive/IE434 Deep Dive 11 project/Deep Dive 3 (Milestone 2)/station_coordinates.pkl')\n"
      ],
      "metadata": {
        "id": "vRc1LhLcDADt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the above pickle files\n",
        "\n",
        "final_df_filename = '/content/drive/My Drive/IE434 Deep Dive 11 project/Deep Dive 3 (Milestone 2)/final_df.pkl'\n",
        "station_coords_df_filename = '/content/drive/My Drive/IE434 Deep Dive 11 project/Deep Dive 3 (Milestone 2)/station_coordinates.pkl'\n",
        "\n",
        "with open(final_df_filename, 'rb') as file:\n",
        "    final_df = pickle.load(file)\n",
        "\n",
        "with open(station_coords_df_filename, 'rb') as file:\n",
        "    station_coords_df = pickle.load(file)\n",
        "\n",
        "print(final_df.head())\n",
        "print(station_coords_df.head())"
      ],
      "metadata": {
        "id": "CidK_0JH_um-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Grouping by Experiments\n",
        "dropping_columns_for_groupby = ['end_station_name','end_lat','end_lng','member_casual','rideable_type']\n",
        "\n",
        "new_df = final_df.drop(columns = dropping_columns_for_groupby)\n",
        "new_df.head()\n",
        "raw_result=new_df.groupby(['start_lat', 'start_lng','start_station_name','date','day_of_week','month','PRCP','SNOW','TMAX','TMIN']).size().reset_index(name='Daily_Demand')\n",
        "result = new_df.groupby(['start_lat', 'start_lng','start_station_name','date','day_of_week','month','PRCP','SNOW','TMAX','TMIN']).size().reset_index(name='Daily_Demand')\n",
        "result.head(20)"
      ],
      "metadata": {
        "id": "fZSnRAGa_3i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim is to group by the dataset by date and start_station to calculate the number of rides starting from a particular station. The station information is given by the numerical values of the latitude and longitudes."
      ],
      "metadata": {
        "id": "Y81KhwnlAKjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(result['start_lat'].values , reverse=True)"
      ],
      "metadata": {
        "id": "5qephTc6_4Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We noticed some outliers (Stations that are very far)in terms of latitude and longitude values and we plan on removing them."
      ],
      "metadata": {
        "id": "9kurWA5sAB_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler1 =MinMaxScaler()\n",
        "print(result.shape)\n",
        "result = result[result['start_lat']<40.755]\n",
        "print(result.shape)\n",
        "\n",
        "lat =np.array(result[['start_lat']])\n",
        "lng =np.array(result[['start_lng']])\n",
        "\n",
        "result['start_lat'] = scaler.fit_transform(result[['start_lat']])\n",
        "result['start_lng'] = scaler1.fit_transform(result[['start_lng']])\n",
        "print(result.shape)\n",
        "plt.plot(result['start_lat'].values)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(result['start_lng'].values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hun6B81I_-nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "sns.histplot(result['Daily_Demand'], bins = 50, kde=True)  # 'kde' adds a Kernel Density Estimate plot\n",
        "plt.title('Distribution of column_name')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YJctPtSrAtAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After looking at the distribution, we decided to make this problem a multi label classification problem. We want to split the demand into three categories, low, Medium and Large - Demand"
      ],
      "metadata": {
        "id": "ppdsrvFwAyYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â€‹We converted the numerical Demand variable into Categorical variable (High, Medium, Low) based onâ€‹ the frequency distribution curve.â€‹\n",
        "\n",
        "â€‹\n",
        "Demand less than 25 percentile (13) is considered as lowâ€‹ and Demand greater than 75 percentile (41) is consideredâ€‹ as high demand and the rest as medium demand."
      ],
      "metadata": {
        "id": "7a-bMFUm58PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "percentile_25 = result['Daily_Demand'].quantile(0.25)\n",
        "percentile_75 = result['Daily_Demand'].quantile(0.75)\n",
        "\n",
        "\n",
        "print(percentile_25)\n",
        "print(percentile_75)"
      ],
      "metadata": {
        "id": "Kqy7nPirAvKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def categorize_demand(value):\n",
        "    if value < percentile_25:\n",
        "        return 'low'\n",
        "    elif percentile_25 <= value <= percentile_75:\n",
        "        return 'med'\n",
        "    else: # value > 41\n",
        "        return 'large'\n",
        "\n",
        "# Assuming 'result' is your DataFrame\n",
        "result['demand_category'] = result['Daily_Demand'].apply(categorize_demand)\n"
      ],
      "metadata": {
        "id": "cWlj1UWMCP70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.drop(columns = ['start_station_name','Daily_Demand','date'],inplace = True)"
      ],
      "metadata": {
        "id": "z8r8qmFlCRko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming 'result' is your DataFrame and 'cat_var1', 'cat_var2' are the categorical variables\n",
        "result = pd.get_dummies(result, columns=['day_of_week', 'month'], drop_first=True)\n",
        "result.head()\n"
      ],
      "metadata": {
        "id": "DPb5W-YeCTz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'result' is your DataFrame\n",
        "\n",
        "# Define the features and the target variable\n",
        "X = result.drop('demand_category', axis=1)  # features (all columns except 'demand_category')\n",
        "y = result['demand_category']  # target variable\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Now X_train and y_train are your training data, and X_test and y_test are your testing data\n"
      ],
      "metadata": {
        "id": "JvRr2SNbCViO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.to_pickle('/content/drive/My Drive/IE434 Deep Dive 11 project/Deep Dive 3 (Milestone 2)/X_train.pkl')\n",
        "X_test.to_pickle('/content/drive/My Drive/IE434 Deep Dive 11 project/Deep Dive 3 (Milestone 2)/X_test.pkl')"
      ],
      "metadata": {
        "id": "U64yLXLFCY31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.to_pickle('/content/drive/My Drive/IE434 Deep Dive 11 project/Deep Dive 3 (Milestone 2)/y_train.pkl')\n",
        "y_test.to_pickle('/content/drive/My Drive/IE434 Deep Dive 11 project/Deep Dive 3 (Milestone 2)/y_test.pkl')"
      ],
      "metadata": {
        "id": "gZ0pidtHCavO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}